{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T05:15:21.164951008Z",
     "start_time": "2023-06-28T05:15:18.724999048Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "    \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import emoji\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import wordcloud\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T05:15:21.189712264Z",
     "start_time": "2023-06-28T05:15:21.166980880Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_TABLES= ['sms','yt','my-collection','spam-word','emoji']\n",
    "MAX_LENGTH =  50\n",
    "MAX_CURRENCY_FLAG = 2\n",
    "MAX_SPAM_WORDS = 1\n",
    "MAX_EMOJI = 2\n",
    "MAX_CONATANS = 1\n",
    "MAX_EMAIL= 1\n",
    "MAX_PHONE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T05:15:21.302374671Z",
     "start_time": "2023-06-28T05:15:21.170055689Z"
    }
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(r\"cyradar_model/dataset.sqlite\")\n",
    "\n",
    "temp = []\n",
    "for i in range(len(DATA_TABLES)):\n",
    "    temp.append(pd.read_sql(f\"SELECT * FROM '{DATA_TABLES[i]}'\",con))\n",
    "con.close()\n",
    "\n",
    "df = pd.concat(temp,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"category\",data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ham  = df[df['category'] == 0].copy()\n",
    "data_spam = df[df['category'] == 1].copy()\n",
    "\n",
    "def show_wordcloud(df, title):\n",
    "    text = ' '.join(df['comment'].astype(str).tolist())\n",
    "    stopwords = set(wordcloud.STOPWORDS)\n",
    "    \n",
    "    fig_wordcloud = wordcloud.WordCloud(stopwords=stopwords,background_color='lightgrey',\n",
    "                    colormap='viridis', width=800, height=600).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(10,7), frameon=True)\n",
    "    plt.imshow(fig_wordcloud)  \n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=20 )\n",
    "    plt.show()\n",
    "show_wordcloud(data_spam, \"Spam messages\")\n",
    "show_wordcloud(data_ham, \"Ham messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T05:15:21.317686459Z",
     "start_time": "2023-06-28T05:15:21.315700660Z"
    }
   },
   "outputs": [],
   "source": [
    "class UpSample:\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        ros = RandomOverSampler(random_state=73133)\n",
    "\n",
    "        x, y = ros.fit_resample(\n",
    "            df[[\"comment\",\"currency\",\"length\",\"spam_word\",\"emoji\",\"contain\",\"email\",\"phone\"]].values\n",
    "            ,df['category'].values\n",
    "        )\n",
    "        df =  pd.DataFrame(x, columns=[\"comment\",\"currency\",\"length\",\"spam_word\",\"emoji\",\"contain\",\"email\",\"phone\"])\n",
    "        df['category'] = y\n",
    "        return  df\n",
    "\n",
    "class ConvertData:\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.drop_duplicates()\n",
    "        df = df.dropna()\n",
    "        df[\"category\"] = df[\"category\"].astype(bool)\n",
    "        df[\"comment\"] = df[\"comment\"].astype(str)\n",
    "        return df\n",
    "\n",
    "\n",
    "class RemoveStopWordsPunctuation:\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def __remove_punctuation_stopwords(self, text):\n",
    "        pattern = re.compile(\"[{}]\".format(re.escape(\"!\\\"#&'()*,-/:;<=>?[\\\\]^_`{|}~\")))\n",
    "        text = \" \".join(\n",
    "            [\n",
    "                word.strip()\n",
    "                for word in pattern.sub(\" \", text.lower()).split()\n",
    "                if word not in set(stopwords.words(\"english\"))\n",
    "            ]\n",
    "        )\n",
    "        return text\n",
    "\n",
    "    def transform(self, df):\n",
    "        df[\"comment\"] = df[\"comment\"].apply(self.__remove_punctuation_stopwords)\n",
    "        return df\n",
    "\n",
    "\n",
    "class AddLengthFlag:\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X[\"length\"] = X[\"comment\"].str.len().astype(np.float32) / MAX_LENGTH\n",
    "        return X\n",
    "\n",
    "\n",
    "class AddCurrencyFlag:\n",
    "    def __init__(self) -> None:\n",
    "        self.currency_symbols = [\"₤\", \"₨\", \"€\", \"₹\", \"₿\", \"$\"]\n",
    "        self.pattern = \"([\\$₤₨€₹₿]+ *[0-9]* *[\\.,]?[0-9]*)|([0-9]* *[\\.,]?[0-9]* *[\\$₤₨€₹₿]+)\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def __add_currency_count(self, text):\n",
    "        return len(re.findall(self.pattern, text)) / MAX_CURRENCY_FLAG\n",
    "\n",
    "    def transform(self, df):\n",
    "        df[\"currency\"] = df[\"comment\"].apply(self.__add_currency_count).astype(np.float32)\n",
    "        return df\n",
    "\n",
    "\n",
    "class AddSpamWordsFlag:\n",
    "    def __init__(self) -> None:\n",
    "        self.spam_words = [\n",
    "            \"urgent\",\n",
    "            \"exclusive\",\n",
    "            \"limited time\",\n",
    "            \"free\",\n",
    "            \"guaranteed\",\n",
    "            \"act now\",\n",
    "            \"discount\",\n",
    "            \"special offer\",\n",
    "            \"prize\",\n",
    "            \"instant\",\n",
    "            \"cash\",\n",
    "            \"save\",\n",
    "            \"win\",\n",
    "            \"best\",\n",
    "            \"secret\",\n",
    "            \"incredible\",\n",
    "            \"congratulations\",\n",
    "            \"approved\",\n",
    "            \"risk free\",\n",
    "            \"hidden\",\n",
    "            \"bonus\",\n",
    "            \"sale\",\n",
    "            \"amazing\",\n",
    "            \"extra cash\",\n",
    "            \"opportunity\",\n",
    "            \"easy\",\n",
    "            \"double your\",\n",
    "            \"best price\",\n",
    "            \"cash back\",\n",
    "            \"deal\",\n",
    "            \"earn\",\n",
    "            \"money\",\n",
    "            \"no obligation\",\n",
    "            \"profit\",\n",
    "            \"results\",\n",
    "            \"exciting\",\n",
    "            \"unbelievable\",\n",
    "            \"jackpot\",\n",
    "            \"fantastic\",\n",
    "            \"instant access\",\n",
    "            \"million dollars\",\n",
    "            \"discounted\",\n",
    "            \"last chance\",\n",
    "            \"exclusive offer\",\n",
    "            \"big savings\",\n",
    "            \"limited offer\",\n",
    "            \"free trial\",\n",
    "            \"special promotion\",\n",
    "            \"secret revealed\",\n",
    "            \"valuable\",\n",
    "            \"money-back guarantee\",\n",
    "            \"lowest price\",\n",
    "            \"save money\",\n",
    "            \"make money\",\n",
    "            \"no risk\",\n",
    "            \"exclusive deal\",\n",
    "            \"limited supply\",\n",
    "            \"huge\",\n",
    "            \"incredible offer\",\n",
    "            \"prize winner\",\n",
    "            \"earn extra income\",\n",
    "            \"limited spots\",\n",
    "            \"new offer\",\n",
    "            \"best deal\",\n",
    "            \"don't miss out\",\n",
    "            \"great savings\",\n",
    "            \"top offer\",\n",
    "            \"double your income\",\n",
    "            \"discount code\",\n",
    "            \"fast cash\",\n",
    "            \"top-rated\",\n",
    "            \"best value\",\n",
    "            \"no cost\",\n",
    "            \"elite\",\n",
    "            \"act fast\",\n",
    "            \"unbeatable\",\n",
    "            \"cash prize\",\n",
    "            \"limited availability\",\n",
    "            \"special discount\",\n",
    "            \"quick cash\",\n",
    "            \"no catch\",\n",
    "            \"instant approval\",\n",
    "            \"big discount\",\n",
    "            \"easy money\",\n",
    "            \"insider\",\n",
    "            \"invitation\",\n",
    "            \"free shipping\",\n",
    "            \"huge discount\",\n",
    "            \"extra income\",\n",
    "            \"secret formula\",\n",
    "            \"no strings attached\",\n",
    "            \"money-making\",\n",
    "            \"dream come true\",\n",
    "            \"massive\",\n",
    "            \"free gift\",\n",
    "            \"incredible opportunity\",\n",
    "            \"risk-free trial\",\n",
    "            \"instant money\",\n",
    "            \"special price\",\n",
    "            \"no purchase necessary\",\n",
    "            \"now\",\n",
    "        ]\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def __add_currency_count(self, text):\n",
    "        return float(sum(text.count(symbol) for symbol in self.spam_words) / MAX_SPAM_WORDS)\n",
    "\n",
    "    def transform(self, df):\n",
    "        df[\"spam_word\"] = df[\"comment\"].apply(self.__add_currency_count).astype(np.float32)\n",
    "        return df\n",
    "\n",
    "\n",
    "class AddEmojiFlag:\n",
    "    def __init__(self) -> None:\n",
    "        self.emoji_symbols = \"[💭|🔝|🆗|🎉|🎊|📯|🙌|😂|💸|👉|📢|🚀|💲|💣|🔱|💼|🆙|⏳|✨|💌|💎|🆕|🔞|💡|💰|👑|⭐|🌟|🎤|⚡|📈|💵|🏆|💪|🔓|🆓|🎰|⌚|🚨|💢|📮|🔥|🎈|🎥|🔔|💯|🎶|🔗|🎁|📚|🔊|👍|👏|📱|📝|🤑|🏅|🔒|📣|💥]\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def __add_currency_count(self, text):\n",
    "        return float(len(re.findall(self.emoji_symbols, text)) / MAX_EMOJI)\n",
    "\n",
    "    def transform(self, df):\n",
    "        df[\"emoji\"] = df[\"comment\"].apply(self.__add_currency_count).astype(np.float32)\n",
    "        return df\n",
    "\n",
    "\n",
    "class AddContainFlag:\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def __add_first_count(self, text):\n",
    "        pattern = \"[0-9]*%|T&C\"\n",
    "        return len(re.findall(pattern, text))\n",
    "\n",
    "    def __add_second_count(self, text):\n",
    "        pattern = \"(https:\\/\\/www\\.|http:\\/\\/www\\.|https:\\/\\/|http:\\/\\/)?[a-zA-Z0-9]{2,}(\\.[a-zA-Z0-9]{2,})(\\.[a-zA-Z0-9]{2,})?\"\n",
    "        return len(re.findall(pattern, text))\n",
    "\n",
    "    def transform(self, df):\n",
    "        df[\"contain\"] = df[\"comment\"].apply(self.__add_first_count)\n",
    "        df[\"contain\"] = df[\"contain\"] + df[\"comment\"].apply(self.__add_second_count)\n",
    "        df['contain'] = df['contain'].astype(np.float32) / MAX_CONATANS\n",
    "        return df\n",
    "\n",
    "\n",
    "class AddEmailFlag:\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def __add_email_count(self, text):\n",
    "        pattern = \"[\\w]+@[\\w]+\\.\\w+\"\n",
    "        return float(len(re.findall(pattern, text))  /MAX_EMAIL)\n",
    "\n",
    "    def transform(self, df):\n",
    "        df[\"email\"] = df[\"comment\"].apply(self.__add_email_count).astype(np.float32)\n",
    "        return df\n",
    "\n",
    "\n",
    "class AddPhoneFlag:\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def __add_phone_no_count(self, text):\n",
    "        pattern = \"\\+?[0-9]?[0-9]? ?0?[0-9]{10}\"\n",
    "        return len(re.findall(pattern, text))\n",
    "\n",
    "    def __add_phone_no_count_1(self, text):\n",
    "        pattern = \"\\+?[0-9]?\\d{3}[ -]?\\d{3}[ -]?\\d{4}\"\n",
    "        return len(re.findall(pattern, text))\n",
    "\n",
    "    def transform(self, df):\n",
    "        df[\"phone\"] = df[\"comment\"].apply(self.__add_phone_no_count)\n",
    "        df[\"phone\"] = df[\"phone\"] + df[\"comment\"].apply(self.__add_phone_no_count_1)\n",
    "        df[\"phone\"] = df[\"phone\"].astype(np.float32) / MAX_PHONE\n",
    "\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class RemovePhoneLinkEmail:\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def __remove(self, text):\n",
    "        text = re.sub(\"\\$[0-9]*([\\.,][0-9]{2})*\\$?\", \"\", text)\n",
    "        text = re.sub(\"\\+?[0-9]?[0-9]? ?0?[0-9]{10}\", \"\", text)\n",
    "        text = re.sub(\"\\+?[0-9]?\\d{3}[ -]?\\d{3}[ -]?\\d{4}\", \"\", text)\n",
    "        text = re.sub(\n",
    "            r\"(https:\\/\\/www\\.|http:\\/\\/www\\.|https:\\/\\/|http:\\/\\/)?[a-zA-Z0-9]{2,}(\\.[a-zA-Z0-9]{2,})(\\.[a-zA-Z0-9]{2,})?\",\n",
    "            \"\",\n",
    "            text,\n",
    "        )\n",
    "        text = re.sub(r\"[\\w]+@[\\w]+\\.\\w+\", \"\", text)\n",
    "        text = emoji.replace_emoji(text)\n",
    "        return text\n",
    "\n",
    "    def transform(self, df):\n",
    "        df[\"comment\"] = df[\"comment\"].apply(self.__remove)\n",
    "        return df\n",
    "\n",
    "\n",
    "class LemmatizeText:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def __lemmatize_text(self, text):\n",
    "        return \" \".join(\n",
    "            [self.lemmatizer.lemmatize(word) for word in re.split(\"\\W+\", text)]\n",
    "        ).strip()\n",
    "\n",
    "    def transform(self, df):\n",
    "        df[\"comment\"] = df[\"comment\"].map(lambda text: self.__lemmatize_text(text))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T05:15:21.411228115Z",
     "start_time": "2023-06-28T05:15:21.318628854Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pipe =  Pipeline([\n",
    "    (\"ConvertData\",ConvertData()),\n",
    "    \n",
    "\n",
    "    (\"AddCurrencyFlag\",AddCurrencyFlag()),\n",
    "    (\"AddSpamWordsFlag\",AddSpamWordsFlag()),\n",
    "    (\"AddEmojiFlag\",AddEmojiFlag()),\n",
    "    (\"AddContainFlag\",AddContainFlag()),\n",
    "    (\"AddEmailFlag\",AddEmailFlag()),\n",
    "    (\"AddPhoneFlag\",AddPhoneFlag()),\n",
    "\n",
    "    (\"RemovePhoneLinkEmail\",RemovePhoneLinkEmail()),\n",
    "    (\"RemoveStopWordsPunctuation\",RemoveStopWordsPunctuation()),\n",
    "    \n",
    "    (\"LemmatizeText\",LemmatizeText()),\n",
    "\n",
    "    (\"AddLengthFlag\",AddLengthFlag()),\n",
    "    (\"UpSample\",UpSample())\n",
    "\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T05:15:50.212700346Z",
     "start_time": "2023-06-28T05:15:21.411782994Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pipe.transform(df)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T05:15:50.606130979Z",
     "start_time": "2023-06-28T05:15:50.200881800Z"
    }
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.countplot(x=\"currency\",data=df)\n",
    "# sns.countplot(x=\"spam_word\",data=df)\n",
    "# sns.countplot(x=\"emoji\",data=df)\n",
    "# sns.countplot(x=\"contain\",data=df)\n",
    "# sns.countplot(x=\"email\",data=df)\n",
    "# sns.countplot(x=\"phone\",data=df)\n",
    "# sns.countplot(x=\"length\",data=df)\n",
    "sns.countplot(x=\"category\",data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T05:15:50.606411384Z",
     "start_time": "2023-06-28T05:15:50.596255693Z"
    }
   },
   "outputs": [],
   "source": [
    "y = pd.DataFrame(df.category)\n",
    "x = df.drop([\"category\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T05:15:50.685929455Z",
     "start_time": "2023-06-28T05:15:50.600348889Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.8,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = {\n",
    "    \"Comment\": tf.convert_to_tensor(x_train[\"comment\"]),\n",
    "    \"Length\": tf.convert_to_tensor(x_train[\"length\"], dtype=tf.float32),\n",
    "    \"Currency\": tf.convert_to_tensor(x_train[\"currency\"], dtype=tf.float32),\n",
    "    \"Spam Words\": tf.convert_to_tensor(x_train[\"spam_word\"], dtype=tf.float32),\n",
    "    \"Emoji\": tf.convert_to_tensor(x_train[\"emoji\"], dtype=tf.float32),\n",
    "    \"Contain\": tf.convert_to_tensor(x_train[\"contain\"], dtype=tf.float32),\n",
    "    \"Email\": tf.convert_to_tensor(x_train[\"email\"], dtype=tf.float32),\n",
    "    \"Phone\": tf.convert_to_tensor(x_train[\"phone\"], dtype=tf.float32)\n",
    "}\n",
    "\n",
    "X_test={\n",
    "    \"Comment\": tf.convert_to_tensor(x_test[\"comment\"]),\n",
    "    \"Length\": tf.convert_to_tensor(x_test[\"length\"], dtype=tf.float32),\n",
    "    \"Currency\": tf.convert_to_tensor(x_test[\"currency\"], dtype=tf.float32),\n",
    "    \"Spam Words\": tf.convert_to_tensor(x_test[\"spam_word\"], dtype=tf.float32),\n",
    "    \"Emoji\": tf.convert_to_tensor(x_test[\"emoji\"], dtype=tf.float32),\n",
    "    \"Contain\": tf.convert_to_tensor(x_test[\"contain\"], dtype=tf.float32),\n",
    "    \"Email\": tf.convert_to_tensor(x_test[\"email\"], dtype=tf.float32),\n",
    "    \"Phone\": tf.convert_to_tensor(x_test[\"phone\"], dtype=tf.float32)\n",
    "}\n",
    "\n",
    "y_train = { \"Spam\" : tf.convert_to_tensor(y_train) }\n",
    "y_test = { \"Spam\" : tf.convert_to_tensor(y_test) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T05:16:02.350694027Z",
     "start_time": "2023-06-28T05:15:50.771914798Z"
    }
   },
   "outputs": [],
   "source": [
    "string_input = tf.keras.layers.Input(shape=[], dtype=tf.string , name=\"Comment\")\n",
    "length_input   = tf.keras.layers.Input(shape=(1,),name=\"Length\",dtype=tf.float32)\n",
    "currency_input = tf.keras.layers.Input(shape=(1,),name=\"Currency\",dtype=tf.float32)\n",
    "spam_word_input = tf.keras.layers.Input(shape=(1,),name=\"Spam Words\",dtype=tf.float32)\n",
    "emoji_input = tf.keras.layers.Input(shape=(1,),name=\"Emoji\",dtype=tf.float32)\n",
    "contain_input = tf.keras.layers.Input(shape=(1,),name=\"Contain\",dtype=tf.float32)\n",
    "email_input = tf.keras.layers.Input(shape=(1,),name=\"Email\",dtype=tf.float32)\n",
    "phone_input = tf.keras.layers.Input(shape=(1,),name=\"Phone\",dtype=tf.float32)\n",
    "\n",
    "# offline file\n",
    "file_or_url = \"./models/nnlm/\"\n",
    "\n",
    "#online file\n",
    "file_or_url = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "\n",
    "hub_layer = hub.KerasLayer(file_or_url, dtype=tf.string, trainable=True,name=\"NNLM_Hub\")\n",
    "\n",
    "embedding_layer = hub_layer(string_input)\n",
    "\n",
    "def CommonLayer(units:int,layer,name=None):    \n",
    "    s1= tf.keras.layers.Dense(units=units,name = name,activation='relu')(layer)\n",
    "    return s1\n",
    "\n",
    "string_layer = CommonLayer(2500,embedding_layer)\n",
    "string_layer = CommonLayer(2000,string_layer)\n",
    "string_layer = CommonLayer(1500,string_layer)\n",
    "string_layer = CommonLayer(1000,string_layer)\n",
    "string_layer = CommonLayer(500,string_layer)\n",
    "string_layer = tf.keras.layers.BatchNormalization()(string_layer)\n",
    "\n",
    "\n",
    "length_layer = CommonLayer(256,length_input,\"length_layer\")\n",
    "length_layer = CommonLayer(120,length_layer)\n",
    "\n",
    "\n",
    "currency_layer = CommonLayer(256,currency_input,\"currency_layer\")\n",
    "currency_layer = CommonLayer(62,currency_layer)\n",
    "\n",
    "spam_word_layer = CommonLayer(256,spam_word_input,\"spam_word_layer\")\n",
    "spam_word_layer = CommonLayer(62,spam_word_layer)\n",
    "\n",
    "emoji_layer = CommonLayer(256,emoji_input,\"emoji_layer\")\n",
    "emoji_layer = CommonLayer(62,emoji_layer)\n",
    "\n",
    "contain_layer = CommonLayer(256,contain_input,\"conatian_layer\")\n",
    "contain_layer = CommonLayer(256,contain_layer)\n",
    "\n",
    "email_layer = CommonLayer(256,email_input,\"email_layer\")\n",
    "email_layer = CommonLayer(54,email_layer)\n",
    "\n",
    "\n",
    "phone_layer = CommonLayer(256,phone_input,\"phone_layer\")\n",
    "phone_layer = CommonLayer(124,phone_layer)\n",
    "\n",
    "\n",
    "\n",
    "concat_layer_level1 = tf.keras.layers.concatenate([length_layer,currency_layer,spam_word_layer])\n",
    "concat_layer_level1 = CommonLayer(600,concat_layer_level1)\n",
    "\n",
    "\n",
    "concat_layer_level2 = tf.keras.layers.concatenate([contain_layer,emoji_layer,email_layer,phone_layer])\n",
    "concat_layer_level2 = CommonLayer(800,concat_layer_level2)\n",
    "\n",
    "\n",
    "concat_layer_level = tf.keras.layers.concatenate([concat_layer_level1,concat_layer_level2])\n",
    "\n",
    "sub_layer = CommonLayer(800,concat_layer_level,\"features_layer\")\n",
    "sub_layer = tf.keras.layers.BatchNormalization()(sub_layer)\n",
    "\n",
    "\n",
    "# Concatenate all input branches\n",
    "concat_layer = tf.keras.layers.concatenate([string_layer,sub_layer ])\n",
    "concat_layer  = tf.keras.layers.Dropout(rate=0.2)(concat_layer)\n",
    "\n",
    "# Add dense and output layers\n",
    "f1 = CommonLayer(1000,concat_layer)\n",
    "\n",
    "f1 = CommonLayer(500,f1)\n",
    "f1 = CommonLayer(250,f1)\n",
    "f1 = CommonLayer(150,f1)\n",
    "f1 = CommonLayer(64,f1)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1,activation='relu',name=\"category\")(f1)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=[string_input, length_input,currency_input,spam_word_input,emoji_input,contain_input,email_input,phone_input], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.build([])\n",
    "#67248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T05:16:07.428301366Z",
     "start_time": "2023-06-28T05:16:02.154861559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot model\n",
    "keras.utils.plot_model(model, to_file='model.png',show_shapes=1,expand_nested=1,show_layer_activations=1)\n",
    "\n",
    "# Display the image\n",
    "# data = plt.imread('model.png')\n",
    "# plt.imshow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k = model.fit(X_train,\n",
    "#           y_train,\n",
    "#           epochs=1,\n",
    "#           batch_size=12,\n",
    "#           validation_data=(X_test, y_test),\n",
    "#           callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)],\n",
    "#           verbose=1\n",
    "# )\n",
    "model = tf.keras.models.load_model('../cyberadar_web/spam-model.h5', custom_objects={'KerasLayer':hub.KerasLayer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([X_test],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = k.history['accuracy']\n",
    "# loss = k.history['loss']\n",
    "# val_accuracy = k.history['val_accuracy']\n",
    "# val_loss = k.history['val_loss']\n",
    "\n",
    "# final_accuracy = accuracy[-1]\n",
    "# final_loss = loss[-1]\n",
    "# final_val_accuracy = val_accuracy[-1]\n",
    "# final_val_loss = val_loss[-1]\n",
    "\n",
    "# print(\"Final Accuracy:\", final_accuracy)\n",
    "# print(\"Final Loss:\", final_loss)\n",
    "# print(\"Final Validation Accuracy:\", final_val_accuracy)\n",
    "# print(\"Final Validation Loss:\", final_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate([X_test],y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = k.history['accuracy']\n",
    "# loss = k.history['loss']\n",
    "# val_accuracy = k.history['val_accuracy']\n",
    "# val_loss = k.history['val_loss']\n",
    "\n",
    "# # Plot the training and validation accuracy\n",
    "# plt.plot(range(1, len(accuracy) + 1), accuracy, label='Training Accuracy')\n",
    "# plt.plot(range(1, len(val_accuracy) + 1), val_accuracy, label='Validation Accuracy')\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot the training and validation loss\n",
    "# plt.plot(range(1, len(loss) + 1), loss, label='Training Loss')\n",
    "# plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
